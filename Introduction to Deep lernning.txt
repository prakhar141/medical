Introduction to 
Deep Learning 

Dr. Asok Bandyopadhyay
Associate Director, Head ICT&S Group
asok.bandyopadhyay@cdac.in

C-DAC/Kolkata 1

www.cdac.in



Lecture Outline

• Machine learning basics
 Supervised and unsupervised learning
 Linear and non-linear classification methods

• Introduction to deep learning
• Elements of neural networks (NNs)

 Activation functions

• Training NNs
 Gradient descent
 Regularization methods 

• NN architectures
 Convolutional NNs
 Recurrent NNs

C-DAC/Kolkata 2

www.cdac.in



Machine Learning Basics
Machine Learning Basics

• Artificial Intelligence is a scientific field concerned with the 
development of algorithms that allow computers to learn without 
being explicitly programmed

• Machine Learning is a branch of Artificial Intelligence, which 
focuses on methods that learn from data and make predictions on 
unseen data

Machine Learning 
Labeled Data algorithm

Training

Prediction

Learned 
Labeled Data Prediction

model

C-DAC/Kolkata 3
Picture from: Ismini Lourentzou – Introduction to Deep Learning

www.cdac.in



Machine Learning Types
Machine Learning Basics

• Supervised: learning with labeled data

– Example: email classification, image classification

– Example: regression for predicting real-valued outputs

• Unsupervised: discover patterns in unlabeled data

– Example: cluster similar data points

• Reinforcement learning: learn to act based on feedback/reward

– Example: learn to play Go  

class A

class B

Classification Regression Clustering

C-DAC/Kolkata 4
Slide credit: Ismini Lourentzou – Introduction to Deep Learning

www.cdac.in



Supervised Learning
Machine Learning Basics

• Supervised learning categories and techniques
– Numerical classifier functions

• Linear classifier, perceptron, logistic regression, support vector 
machines (SVM), neural networks 

– Parametric (probabilistic) functions 
• Naïve Bayes, Gaussian discriminant analysis (GDA), hidden Markov 

models (HMM), probabilistic graphical models 

– Non-parametric (instance-based) functions
• k-nearest neighbors, kernel regression, kernel density estimation, local 

regression

– Symbolic functions
• Decision trees, classification and regression trees (CART)

– Aggregation (ensemble) learning
• Bagging, boosting (Adaboost), random forest 

C-DAC/Kolkata 5
Slide credit: Y-Fan Chang – An Overview of Machine Learning

www.cdac.in



Unsupervised Learning 
Machine Learning Basics

• Unsupervised learning categories and techniques

– Clustering

• k-means clustering

• Mean-shift clustering

• Spectral clustering 

– Density estimation 

• Gaussian mixture model (GMM) 

• Graphical models 

– Dimensionality reduction 

• Principal component analysis (PCA) 

• Factor analysis 

C-DAC/Kolkata 6
Slide credit: Y-Fan Chang – An Overview of Machine Learning

www.cdac.in



Nearest Neighbor Classifier
Machine Learning Basics

• Nearest Neighbor – for each test data point, assign the class label of the nearest 
training data point

– Adopt a distance function to find the nearest neighbor

• Calculate the distance to each data point in the training set, and assign 
the class of the nearest data point (minimum distance)

– It does not require learning a set of weights

Test Training 
Training example examples 

examples from class 
from class 2

1

C-DAC/Kolkata 7
Picture from: James Hays – Machine Learning Overview

www.cdac.in



Nearest Neighbor Classifier
Machine Learning Basics

• For image classification, the distance between all pixels is calculated (e.g., 
using ℓ1 norm, or ℓ2 norm)

– Accuracy on CIFAR-10: 38.6%

• Disadvantages:

– The classifier must remember all training data and store it for future 
comparisons with the test data

– Classifying a test image is expensive since it requires a comparison to 
all training images

ℓ1 norm
(Manhattan distance)

C-DAC/Kolkata 8
Picture from: https://cs231n.github.io/classification/

www.cdac.in



k-Nearest Neighbors Classifier
Machine Learning Basics

• k-Nearest Neighbors approach considers multiple neighboring data points to 
classify a test data point

– E.g., 3-nearest neighbors 

• The test example in the figure is the + mark

• The class of the test example is obtained by voting (based on the 
distance to the 3 closest points)

x2
x

x
x o

x x
x x

+ o
o x

o x
o +

o o
o

o

C-DAC/Kolkata x1 9
Picture from: James Hays – Machine Learning Overview

www.cdac.in



Linear Classifier
Machine Learning Basics

• Linear classifier
– Find a linear function f of the inputs xi that separates the classes

𝑓 𝑥𝑖 ,𝑊, 𝑏 = 𝑊𝑥𝑖 + 𝑏

– Use pairs of inputs and labels to find the weights matrix W and the bias 
vector b

• The weights and biases are the parameters of the function f
– Several methods have been used to find the optimal set of parameters of a 

linear classifier 
• A common method of choice is the Perceptron algorithm, where the 

parameters are updated until a minimal error is reached (single layer, 
does not use backpropagation)

– Linear classifier is a simple approach, but it is a building block of advanced 
classification algorithms, such as SVM and neural networks

• Earlier multi-layer neural networks were referred to as multi-layer 
perceptrons (MLPs)

C-DAC/Kolkata 10

www.cdac.in



Linear Classifier
Machine Learning Basics

• The decision boundary is linear

– A straight line in 2D, a flat plane in 3D, a hyperplane in 
3D and higher dimensional space

• Example: classify an input image

– The selected parameters in this example are not good, 
because the predicted cat score is low

C-DAC/Kolkata 11
Picture from: https://cs231n.github.io/classification/

www.cdac.in



Support Vector Machines
Machine Learning Basics

• Support vector machines (SVM)
– How to find the best decision boundary?

• All lines in the figure correctly separate the 2 classes

• The line that is farthest from all training examples will 
have better generalization capabilities

– SVM solves an optimization problem:
• First, identify a decision boundary that correctly 

classifies the examples
o Next, increase the geometric margin between the 

boundary and all examples 

 The data points that define the maximum 
margin width are called support vectors

 Find W and b by solving:

C-DAC/Kolkata 12

www.cdac.in



Linear vs Non-linear Techniques
Linear vs Non-linear Techniques

• Linear classification techniques
– Linear classifier
– Perceptron
– Logistic regression
– Linear SVM
– Naïve Bayes

• Non-linear classification techniques
– k-nearest neighbors
– Non-linear SVM
– Neural networks
– Decision trees
– Random forest

C-DAC/Kolkata 13

www.cdac.in



Linear vs Non-linear Techniques
Linear vs Non-linear Techniques

• For some tasks, input 
data can be linearly 
separable, and linear 
classifiers can be 
suitably applied

• For other tasks, linear 
classifiers may have 
difficulties to produce 
adequate decision 
boundaries 

C-DAC/Kolkata 14
Picture from: Y-Fan Chang – An Overview of Machine Learning

www.cdac.in



Non-linear Techniques
Linear vs Non-linear Techniques

• Non-linear classification

– Features 𝑧𝑖 are obtained as non-linear functions of the inputs 𝑥𝑖
– It results in non-linear decision boundaries

– Can deal with non-linearly separable data

Inputs: 𝑥𝑖 = 𝑥𝑛1 𝑥𝑛2

Features: 𝑧𝑖 = 𝑥 2
𝑛1 𝑥𝑛2 𝑥𝑛1 ∙ 𝑥𝑛2 𝑥 𝑥2

𝑛1 𝑛2

Outputs: 𝑓 𝑥𝑖 ,𝑊, 𝑏 = 𝑊𝑧𝑖 + 𝑏
C-DAC/Kolkata 15

Picture from: Y-Fan Chang – An Overview of Machine Learning

www.cdac.in



Non-linear Support Vector Machines
Linear vs Non-linear Techniques

• Non-linear SVM
– The original input space is mapped to a higher-dimensional feature space 

where the training set is linearly separable

– Define a non-linear kernel function to calculate a non-linear decision 
boundary in the original feature space

Φ: 𝑥 ↦ 𝜙 𝑥

C-DAC/Kolkata 16
Picture from: James Hays – Machine Learning Overview

www.cdac.in



Binary vs Multi-class Classification
Binary vs Multi-class Classification

• A classification problem with only 2 classes is referred to as binary 
classification

– The output labels are 0 or 1

– E.g., benign or malignant tumor, spam or no-spam email

• A problem with 3 or more classes is referred to as multi-class classification

C-DAC/Kolkata 17

www.cdac.in



Binary vs Multi-class Classification
Binary vs Multi-class Classification

• Both the binary and multi-class classification 
problems can be linearly or non-linearly separated
– Figure: linearly and non-linearly separated data for 

binary classification problem

C-DAC/Kolkata 18

www.cdac.in



Computer Vision Tasks
Machine Learning Basics

• Computer vision has been the primary area of 
interest for ML

• The tasks include: classification, localization, 
object detection, instance segmentation

C-DAC/Kolkata 19
Picture from: Fie-Fei Li, Andrej Karpathy, Justin Johnson – Understanding and Visualizing CNNs 

www.cdac.in



No-Free-Lunch Theorem
Machine Learning Basics

• Wolpert (2002) - The Supervised Learning No-Free-
Lunch Theorems

• The derived classification models for supervised 
learning are simplifications of the reality 
– The simplifications are based on certain assumptions
– The assumptions fail in some situations

• E.g., due to inability to perfectly estimate ML model parameters 
from limited data

• In summary, No-Free-Lunch Theorem states:
– No single classifier works the best for all possible 

problems
– Since we need to make assumptions to generalize

C-DAC/Kolkata 20

www.cdac.in



ML vs. Deep Learning
Introduction to Deep Learning

• Conventional machine learning methods rely on human-designed feature 
representations

– ML becomes just optimizing weights to best make a final prediction

C-DAC/Kolkata 21
Picture from: Ismini Lourentzou – Introduction to Deep Learning

www.cdac.in



ML vs. Deep Learning
Introduction to Deep Learning

• Deep learning (DL) is a machine learning subfield that uses multiple layers for 
learning data representations

– DL is exceptionally effective at learning patterns

C-DAC/Kolkata 22
Picture from: https://www.xenonstack.com/blog/static/public/uploads/media/machine-learning-vs-deep-learning.png

www.cdac.in



ML vs. Deep Learning
Introduction to Deep Learning

• DL applies a multi-layer process for learning rich hierarchical  features (i.e., 
data representations)

– Input image pixels → Edges → Textures → Parts → Objects

Low- Mid- High- Trainabl
Output

Level Level Level e 
Features Features Features Classifie

r

C-DAC/Kolkata 23
Slide credit: Param Vir Singh – Deep Learning

www.cdac.in



Why is DL Useful?
Introduction to Deep Learning

• DL provides a flexible, learnable framework for representing visual, text, 
linguistic information

– Can learn in supervised and unsupervised manner

• DL represents an effective end-to-end learning system

• Requires large amounts of training data

• Since about 2010, DL has outperformed other ML techniques

– First in vision and speech, then NLP, and other applications

What Are the Advantages of Using Deep Learning?

• Maximum Utilization of Unstructured Data

• - Elimination of the Need for Feature Engineering

• Ability to Deliver High-Quality Results

• Elimination of Unnecessary Costs

• Elimination of the Need for Data Labelling
C-DAC/Kolkata 24

www.cdac.in



Representational Power
Introduction to Deep Learning

• NNs with at least one hidden layer are universal 
approximators
– Given any continuous function h(x) and some 𝜖 > 0, there exists a 

NN with one hidden layer (and with a reasonable choice of non-
linearity) described with the function f(x), such that ∀𝑥,  ℎ 𝑥 −

• NNs use nonlinear mapping of the inputs x to the 
outputs f(x) to compute complex decision boundaries

• But then, why use deeper NNs?
 The fact that deep NNs work better is an empirical 

observation
 Mathematically, deep NNs have the same 

representational power as a one-layer NN

C-DAC/Kolkata 25

www.cdac.in



Introduction to Neural Networks 
Introduction to Neural Networks

• Handwritten digit recognition (MNIST dataset)
– The intensity of each pixel is considered an input element
– Output is the class of the digit

Input Output

x 0y1 .1 is 1

x2 0y.27 is 2
The image is  

“2”

x256 0y.12 is 0
16 x 16 = 0
256 Ink → 1 Each dimension represents the 

No ink → 0 confidence of a digit
C-DAC/Kolkata 26

Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…

…
…



Introduction to Neural Networks 
Introduction to Neural Networks

• Handwritten digit recognition

x1 y1

x2

Machine y2 “2”

x 256 10
256 𝑓: 𝑅 → 𝑅 y1

0
The function 𝑓 is represented by a neural network

C-DAC/Kolkata 27
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…



Neural Network
neuron

Input Layer 1 Layer 2 Layer L Output

x …… y1
1

x2 …… y2

x ……
N yM

Input  Output  
Layer Hidden Layers Layer

Deep means many hidden layers
C-DAC/Kolkata 28

www.cdac.in

……

……

……

……

……



Elements of Neural Networks 
Introduction to Neural Networks

• NNs consist of hidden layers with neurons (i.e., computational units)

• A single neuron maps a set of inputs into an output number, or 𝑓: 𝑅𝐾 → 𝑅

z  a
a 1w1  a2w2  aK wK  b

1 w1 𝑎 = 𝜎 𝑧
a w2

2 z
  z a

wK output

a Activation 
K weight function

s b
input

bias

C-DAC/Kolkata 29
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…

…



Elements of Neural Networks 
Introduction to Neural Networks

• A NN with one hidden layer and one output layer
Weights Biases

𝒉𝒊𝒅𝒅𝒆𝒏 𝒍𝒂𝒚𝒆𝒓 𝒉 = 𝝈(𝐖𝟏𝒙 + 𝒃𝟏)

𝒐𝒖𝒕𝒑𝒖𝒕 𝒍𝒂𝒚𝒆𝒓 𝒚 = 𝝈(𝑾𝟐𝒉 + 𝒃𝟐)

Activation functions

4 + 2 = 6 neurons (not counting 
𝒚 inputs)

[3 × 4] + [4 × 2] = 20 weights 
𝒙 4 + 2 = 6 biases

26 learnable parameters
𝒉

C-DAC/Kolkata 30
Slide credit: Ismini Lourentzou – Introduction to Deep Learning

www.cdac.in



Elements of Neural Networks 
Introduction to Neural Networks

• Deep NNs have many hidden layers

– Fully-connected (dense) layers (a.k.a. Multi-Layer Perceptron or MLP)

– Each neuron is connected to all neurons in the succeeding layer

Input Layer Layer Layer Output

x 1 2 … L y
1 1

…
x2 … y2

…

x …
N yM

…
Input Output 
Layer Hidden Layer

C-DAC/Kolkata Layers 31
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…

…
…

…
…

…
…



Elements of Neural Networks 
Introduction to Neural Networks

1 4 0.98
1

-
2 1

- - 0.12
-

11 2
1 0

Sigmoid
1 ∙ −1 + −1 ∙ 1 + 0 =-2 z

Function
z 1
 

1 ez
C-DAC/Kolkata z 32

www.cdac.in



Example of Neural Network

1 4 0.98 2 0.86 3 0.62
1

-2 -1 -1
1 0 -2

-1 -2 0.12 -2 0.11 -1 0.83
-1

1 -1 4
0 0 2

C-DAC/Kolkata 33

www.cdac.in



Elements of Neural Networks 
Introduction to Neural Networks

• A simple network, toy example (cont’d)

– For an input vector [1 −1]𝑇, the output is [0.62 0.83]𝑇

1 4 0.98 2 0.86 3 0.62
1

-2 -1 -1
1 0 -2

-1 -2 0.12 -2 0.11 -1 0.83
-1

1 -1 4
0 0 2

1 0.62
𝑓 =

−1 0.83
C-DAC/Kolkata 34

Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Example of Neural Network

1 0.73 2 0.72 3 0.51
0

-2 -1 -1
1 0 -2

-1 0.5 -2 0.12 -1 0.85
0

1 -1 4
0 0 2

1 0. 2
𝑓 = 6 0 0.51

𝑓: 𝑅2 → 𝑅2 𝑓 =
−1 0.83 0 0.85

Different parameters define different function
C-DAC/Kolkata 35

www.cdac.in



Matrix Operation
Introduction to Neural Networks

• Matrix operations are helpful when working with 
multidimensional inputs and outputs

1 4 0.98
1 𝜎 W x + b = a

-
2 1

1 −2 1 1 0.98
-1 -2 0.12 𝜎 −1 1 −1 + 0 = 0.12

-1
1

4
0

−2

C-DAC/Kolkata 36
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Matrix Operation
Introduction to Neural Networks

• Multilayer NN, matrix calculations for the first layer

– Input vector x, weights matrix W1, bias vector b1, output vector a1

x … y
1 1

…
x2 W1 … y2

b …
1

xN x a … yM
1

…

a
1 = 𝜎 W1 x + b

1

C-DAC/Kolkata 37
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…

…
…

…
…

…
…



Matrix Operation
Introduction to Neural Networks

• Multilayer NN, matrix calculations for all layers
x … y

1 1

…
x2 W1 W2 …WL y

b b b 2
1 2 … L

xN x a a … yM
1 2 y

…

𝜎 W1 x + b
1

𝜎 W2 a + b
1 2

𝜎 WL aL-1 + b
L

C-DAC/Kolkata 38
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…

…
…

…
…

…
…



Matrix Operation
Introduction to Neural Networks

• Multilayer NN, function f maps inputs x to outputs y, i.e., 𝑦 = 𝑓(𝑥)

x … y
1 1

…
x2 W1 W2 …WL y

b b b 2
1 2 … L

xN x a a … yM
1 2 y

…

y = 𝑓 x = 𝜎 WL … 𝜎 W2 b b
𝜎 W1 x b

+ 1 + 2 …+ L

C-DAC/Kolkata 39
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…

…
…

…
…

…
…



Softmax Layer
Introduction to Neural Networks

• In multi-class classification tasks, the output layer is typically a softmax layer

– I.e., it employs a softmax activation function

– If a layer with a sigmoid activation function is used as the output layer 
instead, the predictions by the NN may not be easy to interpret

• Note that an output layer with sigmoid activations can still be used for 
binary classification 

A Layer with Sigmoid Activations
3 0.95

z  y1  z1 1 

1 0.73
z2  y2  z2 

- 0.05
z3  y3  z3 3

C-DAC/Kolkata 40
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Softmax Layer
Introduction to Neural Networks

• The softmax layer applies softmax activations to 
output a probability value in the range [0, 1] Probability:

 0 < 𝑦
– The values z inputted to the softmax layer are 𝑖 < 1

  𝑖 𝑦𝑖 = 1
referred to as logits

A Softmax Layer
3

3 z 20 0.88
 z z

z e e 1 y e 1 e j

1 1 
j1

1
z z 2.7 0.12 3

2 e e 2  z z
y2  e 2 e j

j1

- 0.05 ≈0 3
z3 e z

e 3  z z
3

3 y3  e e j

3 j1
z

 e j

j1
C-DAC/Kolkata 41

Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Activation Functions
Introduction to Neural Networks

• Non-linear activations are needed to learn complex (non-
linear) data representations
– Otherwise, NNs would be just a linear function (such as W1W2𝑥 =

𝑊𝑥) 
– NNs with large number of layers (and neurons) can approximate 

more complex functions 
• Figure: more neurons improve representation (but, may overfit)

C-DAC/Kolkata 42
Picture from: http://cs231n.github.io/assets/nn1/layer_sizes.jpeg

www.cdac.in



Activation: Sigmoid
Introduction to Neural Networks

• Sigmoid function σ: takes a real-valued number and “squashes” it into the 
range between 0 and 1

– The output can be interpreted as the firing rate of a biological neuron

• Not firing = 0; Fully firing = 1

– When the neuron’s activation are 0 or 1, sigmoid neurons saturate

• Gradients at these regions are almost zero (almost no signal will flow) 

– Sigmoid activations are less common in modern NNs

ℝ𝑛
𝑓 𝑥 → 0,1

C-DAC/Kolkata 𝑥 43
Slide credit: Ismini Lourentzou – Introduction to Deep Learning

www.cdac.in



Activation: Tanh
Introduction to Neural Networks

• Tanh function: takes a real-valued number and “squashes” it into range 
between -1 and 1

– Like sigmoid, tanh neurons saturate

– Unlike sigmoid, the output is zero-centered

• It is therefore preferred than sigmoid

– Tanh is a scaled sigmoid: tanh(𝑥) = 2 ∙ 𝜎(2𝑥) − 1

ℝ𝑛
𝑓 𝑥 → −1,1

𝑥
C-DAC/Kolkata 44

Slide credit: Ismini Lourentzou – Introduction to Deep Learning

www.cdac.in



Activation: ReLU
Introduction to Neural Networks

• ReLU (Rectified Linear Unit): takes a real-valued number and thresholds it at 
zero

ℝ𝑛 → ℝ𝑛
+

𝑓 𝑥 = max(0, 𝑥)
 Most modern deep NNs use ReLU

activations 
 ReLU is fast to compute 𝑓 𝑥

o Compared to sigmoid, tanh 
o Simply threshold a matrix at zero

 Accelerates the convergence of gradient 
descent
o Due to linear, non-saturating form 

 Prevents the gradient vanishing problem 𝑥

C-DAC/Kolkata 45

www.cdac.in



Activation: Leaky ReLU
Introduction to Neural Networks

• The problem of ReLU activations: they can “die”

– ReLU could cause weights to update  in a way that the gradients can 
become zero and the neuron will not activate again on any data 

– E.g., when a large learning rate is used

• Leaky ReLU activation function is a variant of ReLU

– Instead of the function being 0 when 𝑥 < 0, a leaky ReLU has a 
small negative slope (e.g., α = 0.01, or similar)

𝑓 𝑥
𝛼𝑥 for 𝑥 < 0

 This resolves the dying ReLU problem =  
𝑥 for 𝑥 ≫ 0

 Most current works still use ReLU
o With a proper setting of the learning rate, 

the problem of dying ReLU can be 
avoided

C-DAC/Kolkata 46

www.cdac.in



Activation: Linear Function
Introduction to Neural Networks

• Linear function means that the output signal is proportional to the input signal 
to the neuron ℝ𝑛 → ℝ𝑛

 If the value of the constant c is 1, it is also 
called identity activation function

𝑓 𝑥 = 𝑐𝑥
 This activation type is used in regression 

problems
o E.g., the last layer can have linear 

activation function, in order to output a 
real number (and not a class membership)

C-DAC/Kolkata 47

www.cdac.in



Training NNs
Training Neural Networks

• The network parameters 𝜃 include the weight matrices and bias vectors 
from all layers

𝜃 = 𝑊1, 𝑏1,𝑊2, 𝑏2, ⋯𝑊𝐿 , 𝑏𝐿

– Often, the model parameters 𝜃 are referred to as weights

• Training a model to learn a set of parameters 𝜃 that are optimal 
(according to a criterion) is one of the greatest challenges in ML

x … y is 1
1 0.1

…
x2 … 0y.72 is 2

…

x256 … 0y.2 is 0
1

16 x 16 = 
C-DAC/Kolkata … 0 48

256
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

Softmax

…
…

…
…



Training NNs
Training Neural Networks

• Data preprocessing - helps convergence during 
training
– Mean subtraction, to obtain zero-centered data

• Subtract the mean for each individual data dimension 
(feature)

– Normalization
• Divide each feature by its standard deviation

– To obtain standard deviation of 1 for each data dimension (feature)

• Or, scale the data within the range [0,1] or [-1, 1]
– E.g., image pixel intensities are divided by 255 to be scaled in the 

C-DAC/Kolkata [0,1] range 49
Picture from: https://cs231n.github.io/neural-networks-2/

www.cdac.in



Training NNs
Training Neural Networks

• To train a NN, set the parameters 𝜃 such that for a training subset of images, the 
corresponding elements in the predicted output have maximum values

Input: y1 has the maximum value

Input: y2 has the maximum value
.
.
.

Input: y9 has the maximum value

Input: y10 has the maximum value

C-DAC/Kolkata 50
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Training NNs
Training Neural Networks

• Define a loss function/objective function/cost function ℒ 𝜃 that calculates the 
difference (error) between the model prediction and the true label

– E.g., ℒ 𝜃 can be mean-squared error, cross-entropy, etc.

x … y
1 1 0.2 1

…
x2 … y2 0.3 0

…
Cost 

x ℒ(𝜃)
256 … y1 0.5 0

… 0
True label “1”

C-DAC/Kolkata 51
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…
…
…

…
…

…
…

…
…



Training NNs
Training Neural Networks

• For a training set of 𝑁 images, calculate the total loss overall all images: 
ℒ 𝜃 =  𝑁

𝑛=1ℒ𝑛 𝜃
• Find the optimal parameters 𝜃∗ that minimize the total loss ℒ 𝜃

ℒ1 𝜃
x1 NN 𝑦1 y1

ℒ2 𝜃
x2 NN 𝑦2 y

2

ℒ
3 3 𝜃

x3 NN 𝑦 y3

ℒ𝑛 𝜃
xN NN 𝑦𝑁 yN

C-DAC/Kolkata 52
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…

…
…

…
…



Loss Functions
Training Neural Networks

• Classification tasks

Training 
Pairs of 𝑁 inputs 𝑥𝑖 and ground-truth class labels 𝑦

examples 𝑖

Output Softmax Activations
Layer [maps to a probability distribution]

𝑁 𝐾
1 (𝑖) (𝑖) (𝑖)

Loss function Cross-entropy ℒ 𝜃 = −   𝑦 o 𝑦 + 1 − 𝑦
𝑁 𝑘 l g 𝑘 𝑘 log 1 − 𝑦 𝑖

𝑘
𝑖=1 𝑘=1

Ground-truth class labels 𝑦𝑖 and model predicted class labels 𝑦𝑖

C-DAC/Kolkata 53
Slide credit: Ismini Lourentzou – Introduction to Deep Learning

www.cdac.in



Loss Functions
Training Neural Networks

• Regression tasks
Training 

Pairs of 𝑁 inputs 𝑥𝑖 and ground-truth output values 𝑦𝑖
examples

Output 
Linear (Identity) or Sigmoid Activation

Layer

𝑛
1

Mean Squared Error ℒ 𝜃 =  𝑦(𝑖) 2
− 𝑦(𝑖)

𝑛
Loss 𝑖=1

function 𝑛
1

Mean Absolute Error ℒ 𝜃 =  𝑦(𝑖) − 𝑦(𝑖)
𝑛

𝑖=1

C-DAC/Kolkata 54
Slide credit: Ismini Lourentzou – Introduction to Deep Learning

www.cdac.in



Training NNs
Training Neural Networks

• Optimizing the loss function ℒ 𝜃
– Almost all DL models these days are trained with a variant of the gradient 

descent (GD) algorithm

– GD applies iterative refinement of the network parameters 𝜃
– GD uses the opposite direction of the gradient of the loss with respect to the 

NN parameters (i.e.,𝛻ℒ 𝜃 = 𝜕ℒ 𝜕𝜃𝑖 ) for updating  𝜃
• The gradient of the loss function 𝛻ℒ 𝜃 gives the direction 

of fastest increase of the loss function ℒ 𝜃 when the 
parameters 𝜃 are changed

ℒ 𝜃 𝜕ℒ
𝜕𝜃𝑖

C-DAC/Kolkata
𝜃 55
𝑖

www.cdac.in



Gradient Descent Algorithm
Training Neural Networks

• Steps in the gradient descent algorithm:

1. Randomly initialize the model parameters, 𝜃0

2. Compute the gradient of the loss function at the initial parameters 𝜃0: 𝛻ℒ 𝜃0

3. Update the parameters as: 𝜃𝑛𝑒𝑤 = 𝜃0 − 𝛼𝛻ℒ 𝜃0

• Where α is the learning rate

4. Go to step 2 and repeat (until a terminating criterion is reached)

Loss ℒ Initial 𝜕ℒ
Gradient 𝛻ℒ =

parameters 𝜕𝜃

𝜃0

Parameter update: 𝜃𝑛𝑒𝑤= 𝜃 − 𝛼𝛻ℒ 𝜃0

Global loss minimum ℒ𝑚𝑖𝑛

Parameters 𝜃
C-DAC/Kolkata 56

www.cdac.in



Gradient Descent Algorithm
Training Neural Networks

• Example: a NN with only 2 parameters 𝑤1 and 𝑤2, i.e., 𝜃 = 𝑤1, 𝑤2

– The different colors represent the values of the loss (minimum loss 𝜃∗ is 
≈ 1.3)

1. Randomly pick a 
starting point 𝜃0

2. Compute the 
gradient at 𝜃0, 

𝜃∗ 𝛻ℒ 𝜃0

3. Times the learning 
𝑤2 𝜃1 rate 𝜂, and update 𝜃,

𝜃𝑛𝑒𝑤 = 𝜃0 − 𝛼𝛻ℒ 𝜃0
𝜃1 =

𝜃0 − 𝛼𝛻ℒ 𝜃0 −𝛻ℒ 𝜃0 4. Go to step 2, 

𝜃0 repeat

𝜕ℒ 𝜃0 /𝜕𝑤
𝛻ℒ 𝜃0 = 1

𝜕ℒ 𝜃0 /𝜕𝑤2
C-DAC/Kolkata 57

𝑤1 Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Gradient Descent Algorithm
Training Neural Networks

• Example (contd.)

Eventually, we would reach a minimum …..

2. Compute the 
𝜃2 gradient at 𝜃𝑜𝑙𝑑, 

𝜃1 − 𝛼𝛻ℒ 𝜃1 𝛻ℒ 𝜃𝑜𝑙𝑑

𝜃2 − 𝛼𝛻ℒ 𝜃2 3. Times the learning 
𝑤2 𝜃1 rate 𝜂, and update 𝜃,

𝜃𝑛𝑒𝑤 = 𝜃𝑜𝑙𝑑 − 𝛼𝛻ℒ 𝜃𝑜𝑙𝑑

4. Go to step 2, 
repeat

𝜃0

C-DAC/Kolkata 58
𝑤1 Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Gradient Descent Algorithm
Training Neural Networks

• Gradient descent algorithm stops when a local minimum of the loss surface is 
reached

– GD does not guarantee reaching a global minimum

– However, empirical evidence suggests that GD works well for NNs

ℒ 𝜃

𝜃

C-DAC/Kolkata 59
Picture from: https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/

www.cdac.in



Gradient Descent Algorithm
Training Neural Networks

• For most tasks, the loss surface ℒ 𝜃 is highly complex (and non-convex)

ℒ
• Random initialization in NNs results 

in different initial parameters 𝜃0

every time the NN is trained
 Gradient descent may reach different 

minima at every run
 Therefore, NN will produce different 

predicted outputs 

• In addition, currently we don’t have 
algorithms that guarantee reaching a 𝑤1 𝑤2
global minimum for an arbitrary loss 
function

C-DAC/Kolkata 60
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Backpropagation
Training Neural Networks

• Modern NNs employ the backpropagation method for calculating the gradients of 
the loss function 𝛻ℒ 𝜃 = 𝜕ℒ 𝜕𝜃𝑖
– Backpropagation is short for “backward propagation”

• For training NNs, forward propagation (forward pass) refers to passing the inputs 𝑥
through the hidden layers to obtain the model outputs (predictions) 𝑦
– The loss ℒ 𝑦, 𝑦 function is then calculated 

– Backpropagation traverses the network in reverse order, from the outputs 𝑦
backward toward the inputs 𝑥 to calculate the gradients of the loss 𝛻ℒ 𝜃

– The chain rule is used for calculating the partial derivatives of the loss function 
with respect to the parameters 𝜃 in the different layers in the network

• Each update of the model parameters 𝜃 during training takes one forward and one 
backward pass (e.g., of a batch of inputs)

• Automatic calculation of the gradients (automatic differentiation) is available in all 
current deep learning libraries

– It significantly simplifies the implementation of deep learning algorithms, 
C-DAC/Kolkatasince it obviates deriving the partial derivatives of the loss function by h6a1nd

www.cdac.in



Mini-batch Gradient Descent
Training Neural Networks

• It is wasteful to compute the loss over the entire training dataset 
to perform a single parameter update for large datasets
– E.g., ImageNet has 14M images
– Therefore, GD (a.k.a. vanilla GD) is almost always replaced with 

mini-batch GD
• Mini-batch gradient descent

– Approach:
• Compute the loss ℒ 𝜃 on a mini-batch of images, update the parameters 𝜃, 

and repeat until all images are used
• At the next epoch, shuffle the training data, and repeat the above process

– Mini-batch GD results in much faster training
– Typical mini-batch size: 32 to 256 images
– It works because the gradient from a mini-batch is a good 

approximation of the gradient from the entire training set

C-DAC/Kolkata 62

www.cdac.in



Stochastic Gradient Descent
Training Neural Networks

• Stochastic gradient descent

– SGD uses mini-batches that consist of a single input example

• E.g., one image mini-batch

– Although this method is very fast, it may cause significant fluctuations in the 
loss function

• Therefore, it is less commonly used, and mini-batch GD is preferred

– In most DL libraries, SGD typically means a mini-batch GD (with an option 
to add momentum)

C-DAC/Kolkata 63

www.cdac.in



Problems with Gradient Descent
Training Neural Networks

• Besides the local minima problem, the GD algorithm can be very 
slow at plateaus, and it can get stuck at saddle points

cost ℒ 𝜃

Very slow at the plateau

Stuck at a saddle point

Stuck at a local 
minimum

𝛻ℒ 𝜃 ≈ 0 𝛻ℒ 𝜃
= 0 𝛻ℒ 𝜃 = 0

C-DAC/Kolkata 6𝜃4
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Gradient Descent with Momentum
Training Neural Networks

• Gradient descent with momentum uses the momentum of the gradient for 
parameter optimization

cost ℒ 𝜃
Movement = Negative of Gradient + 
Momentum 

Negative of Gradient

Momentum

Real Movement

𝜃
Gradient = 0

C-DAC/Kolkata 65
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Gradient Descent with Momentum
Training Neural Networks

• Parameters update in GD with momentum at iteration 𝑡: 𝜃𝑡 = 𝜃𝑡−1 − 𝑉𝑡

• Where: 𝑉𝑡= 𝛽𝑉𝑡−1 + 𝛼𝛻ℒ 𝜃𝑡−1

• I.e., 𝜃𝑡 = 𝜃𝑡−1 − 𝛼𝛻ℒ 𝜃𝑡−1 − 𝛽𝑉𝑡−1

• Compare to vanilla GD: 𝜃𝑡 = 𝜃𝑡−1 − 𝛼𝛻ℒ 𝜃𝑡−1

– Where 𝜃𝑡−1 are the parameters from the previous iteration 𝑡 − 1
• The term 𝑉𝑡 is called momentum

– This term accumulates the gradients from the past several steps, i.e., 
𝑉𝑡= 𝛽𝑉𝑡−1 + 𝛼𝛻ℒ 𝜃𝑡−1

= 𝛽 𝛽𝑉𝑡−2 + 𝛼𝛻ℒ 𝜃𝑡−2 + 𝛼𝛻ℒ 𝜃𝑡−1

= 𝛽2𝑉𝑡−2 + 𝛽𝛼𝛻ℒ 𝜃𝑡−2 + 𝛼𝛻ℒ 𝜃𝑡−1

= 𝛽3𝑉𝑡−3 + 𝛽2𝛼𝛻ℒ 𝜃𝑡−3 + 𝛽𝛼𝛻ℒ 𝜃𝑡−2 + 𝛼𝛻ℒ 𝜃𝑡−1

– This term is analogous to a momentum of a heavy ball rolling down the hill 
• The parameter 𝛽 is referred to as a coefficient of momentum

– A typical value of the parameter 𝛽 is 0.9
• This method updates the parameters 𝜃 in the direction of the weighted average 

of the past gradients

C-DAC/Kolkata 66

www.cdac.in



Nesterov Accelerated Momentum
Training Neural Networks

• Gradient descent with Nesterov accelerated 
momentum 
– Parameter update: 𝜃𝑡 = 𝜃𝑡−1 − 𝑉𝑡

• Where: 𝑉𝑡= 𝛽𝑉𝑡−1 + 𝛼𝛻ℒ 𝜃𝑡−1 + 𝛽𝑉𝑡−1

– The term 𝜃𝑡−1 + 𝛽𝑉𝑡−1 allows to predict the position 
of the parameters in the next step (i.e., 𝜃𝑡 ≈ 𝜃𝑡−1 +
𝛽𝑉𝑡−1) GD with 

GD with 
Nesterov

momentum
momentum

– The gradient is calculated with respect to the 
approximate future position of the parameters in the 
next iteration, 𝜃𝑡, calculated at iteration 𝑡 − 1

C-DAC/Kolkata 67
Picture from: https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12

www.cdac.in



Adam
Training Neural Networks

• Adaptive Moment Estimation (Adam)

– Adam combines insights from the momentum optimizers that 
accumulate the values of past gradients, and it also introduces 
new terms based on the second moment of the gradient

• Similar to GD with momentum, Adam computes a 
weighted average of past gradients (first moment of the 
gradient), i.e., 𝑉𝑡= 𝛽1𝑉

𝑡−1 + 1 − 𝛽1 𝛻ℒ 𝜃𝑡−1

• Adam also computes a weighted average of past squared 
gradients (second moment of the gradient), , i.e., 𝑈𝑡= 

𝛽2𝑈
𝑡−1 + 1 − 𝛽2 𝛻ℒ 𝜃𝑡−1 2

C-DAC/Kolkata 68

www.cdac.in



Adam
Training Neural Networks

𝑡
 The parameter update is:𝜃𝑡 = 𝜃𝑡−1 𝑉 

− 𝛼
𝑈 𝑡+𝜖

𝑉𝑡 𝑡
oWhere: 𝑉 𝑡 d 𝑈 𝑡 𝑈

= an =
1−𝛽1 1−𝛽2

oThe proposed default values are 𝛽1 = 0.9, 𝛽2 = 0.999, 
and 𝜖 = 10−8

• Other commonly used optimization methods include:
 Adagrad, Adadelta, RMSprop, Nadam, etc.
 Most commonly used optimizers nowadays are Adam 

and SGD with momentum

C-DAC/Kolkata 69

www.cdac.in



Learning Rate
Training Neural Networks

• Learning rate
– The gradient tells us the direction in which the loss has the steepest rate of 

increase, but it does not tell us how far along the opposite direction we 
should step

– Choosing the learning rate (also called the step size) is one of the most 
important hyper-parameter settings for NN training

LR LR 
too too 
small large

C-DAC/Kolkata 70

www.cdac.in



Learning Rate
Training Neural Networks

• Training loss for different learning rates

– High learning rate: the loss increases or plateaus too quickly

– Low learning rate: the loss decreases too slowly (takes many epochs to 
reach a solution)

C-DAC/Kolkata 71
Picture from: https://cs231n.github.io/neural-networks-3/

www.cdac.in



Learning Rate Scheduling
Training Neural Networks

• Learning rate scheduling is applied to change the values of the 
learning rate during the training

– Annealing is reducing the learning rate over time (a.k.a. 
learning rate decay)

• Approach 1: reduce the learning rate by some factor every 
few epochs

– Typical values: reduce the learning rate by a half every 5 epochs, 
or divide by 10 every 20 epochs

C-DAC/Kolkata 72

www.cdac.in



Learning Rate Scheduling
Training Neural Networks

• Approach 2: exponential or cosine decay gradually reduce the 
learning rate over time

• Approach 3: reduce the learning rate by a constant (e.g., by 
half) whenever the validation loss stops improving

– In TensorFlow: tf.keras.callbacks.ReduceLROnPleateau()
» Monitor: validation loss, factor: 0.1 (i.e., divide by 10), patience: 

10 (how many epochs to wait before applying it), Minimum 
learning rate: 1e-6 (when to stop)

– Warmup is gradually increasing the learning rate initially, and 
afterward let it cool down until the end of the training

Exponential decay Cosine decay Warmup

C-DAC/Kolkata 73

www.cdac.in



Vanishing Gradient Problem
Training Neural Networks

• In some cases, during training, the gradients can become either very small 
(vanishing gradients) of very large (exploding gradients)

– They result in very small or very large update of the parameters

– Solutions: change learning rate, ReLU activations, regularization, LSTM 
units in RNNs

x … y
1 1

…
x2 … y2

…

xN … yM
…

Small gradients, learns very slow

C-DAC/Kolkata 74
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

…
…

…
…

…
…

…
…

…
…



Generalization
Generalization

• Underfitting
– The model is too “simple” to represent 

all the relevant class characteristics

– E.g., model with too few parameters

– Produces high error on the training set 
and high error on the validation set

• Overfitting

– The model is too “complex” and fits 
irrelevant characteristics (noise) in 
the data

– E.g., model with too many 
parameters

– Produces low error on the training 
error and high error on the validation 
set

C-DAC/Kolkata 75

www.cdac.in



Overfitting
Generalization

• Overfitting – a model with high capacity fits the noise in the data 
instead of the underlying relationship

• The model may fit the training data 
very well, but fails to generalize to new 
examples (test or validation data)

C-DAC/Kolkata 76
Picture from: http://cs231n.github.io/assets/nn1/layer_sizes.jpeg

www.cdac.in



Regularization: Weight Decay
Regularization

• ℓ𝟐 weight decay
– A regularization term that penalizes large weights is added to the loss 

function
Data loss Regularization loss

ℒ𝑟𝑒𝑔 𝜃 = ℒ 𝜃 + 𝜆 𝜃2
𝑘

𝑘

– For every weight in the network, we add the regularization term to the loss 
value

• During gradient descent parameter update, every weight is decayed 
linearly toward zero

– The weight decay coefficient 𝜆 determines how dominant the regularization 
is during the gradient computation

C-DAC/Kolkata 77

www.cdac.in



Regularization: Weight Decay
Regularization

• Effect of the decay coefficient 𝜆
– Large weight decay coefficient → penalty for weights with 

large values

C-DAC/Kolkata 78

www.cdac.in



Regularization: Weight Decay
Regularization

• ℓ𝟏 weight decay
– The regularization term is based on the ℓ1 norm of the 

weights

ℒ𝑟𝑒𝑔 𝜃 = ℒ 𝜃 + 𝜆 𝑘 𝜃𝑘

– ℓ1 weight decay is less common with NN
• Often performs worse than ℓ2 weight decay

– It is also possible to combine ℓ1 and ℓ2 regularization 
• Called elastic net regularization

ℒ𝑟𝑒𝑔 𝜃 = ℒ 𝜃 + 𝜆1 𝑘 𝜃𝑘 + 𝜆 2
2  𝑘 𝜃𝑘

C-DAC/Kolkata 79

www.cdac.in



Regularization: Dropout
Regularization

• Dropout
– Randomly drop units (along with their connections) during training

– Each unit is retained with a fixed dropout rate p, independent of other units 

– The hyper-parameter p needs to be chosen (tuned)

• Often, between 20% and 50% of the units are dropped

C-DAC/Kolkata 80
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Regularization: Dropout
Regularization

• Dropout is a kind of ensemble learning
– Using one mini-batch to train one network with a slightly different architecture

minibatch minibatch minibatch minibatch
1 2 3 n

C-DAC/Kolkata 81
Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in

……



Regularization: Early Stopping
Regularization

• Early-stopping
– During model training, use a validation set

• E.g., validation/train ratio of about 25% to 75%

– Stop when the validation accuracy (or loss) has not improved after n
epochs

• The parameter n is called patience

Stop training

validation

C-DAC/Kolkata 82

www.cdac.in



Batch Normalization
Regularization

• Batch normalization layers act similar to the data preprocessing steps 
mentioned earlier

– They calculate the mean μ and variance σ of a batch of input data, and 
normalize the data x to a zero mean and unit variance

𝑥−𝜇
– I.e., 𝑥 =

𝜎

• BatchNorm layers alleviate the problems of proper initialization of the 
parameters and hyper-parameters

– Result in faster convergence training, allow larger learning rates

– Reduce the internal covariate shift

• BatchNorm layers are inserted immediately after convolutional layers or fully-
connected layers, and before activation layers

– They are very common with convolutional NNs

C-DAC/Kolkata 83

www.cdac.in



Hyper-parameter Tuning
Hyper-parameter Tuning

• Training NNs can involve setting many hyper-parameters

• The most common hyper-parameters include:

– Number of layers, and number of neurons per layer

– Initial learning rate

– Learning rate decay schedule (e.g., decay constant)

– Optimizer type

• Other hyper-parameters may include:

– Regularization parameters (ℓ2 penalty, dropout rate)

– Batch size

– Activation functions

– Loss function
C-DAC/Kolkata 84

• Hyper-parameter tuning can be time-consuming for larger NNs

www.cdac.in



Hyper-parameter Tuning
Hyper-parameter Tuning

• Grid search

– Check all values in a range with a step value 

• Random search

– Randomly sample values for the parameter

– Often preferred to grid search

• Bayesian hyper-parameter optimization

– Is an active area of research

C-DAC/Kolkata 85

www.cdac.in



k-Fold Cross-Validation
k-Fold Cross-Validation

• Using k-fold cross-validation for hyper-parameter tuning is 
common when the size of the training data is small

– It also leads to a better and less noisy estimate of the model 
performance by averaging the results across several folds

• E.g., 5-fold cross-validation (see the figure on the next slide)

1. Split the train data into 5 equal folds

2. First use folds 2-5 for training and fold 1 for validation

3. Repeat by using fold 2 for validation, then fold 3, fold 4, and 
fold 5

4. Average the results over the 5 runs (for reporting purposes)

5. Once the best hyper-parameters are determined, evaluate the 

C-DAC/Kolkata model on the test data 86

www.cdac.in



k-Fold Cross-Validation
k-Fold Cross-Validation

• Illustration of a 5-fold cross-validation

C-DAC/Kolkata 87
Picture from: https://scikit-learn.org/stable/modules/cross_validation.html

www.cdac.in



Ensemble Learning
Ensemble Learning

• Ensemble learning is training multiple classifiers separately 
and combining their predictions 
– Ensemble learning often outperforms individual classifiers
– Better results obtained with higher model variety in the ensemble
– Bagging (bootstrap aggregating)

• Randomly draw subsets from the training set (i.e., bootstrap samples)
• Train separate classifiers on each subset of the training set
• Perform classification based on the average vote of all classifiers

– Boosting
• Train a classifier, and apply weights on the training set (apply higher 

weights on misclassified examples, focus on “hard examples”)
• Train new classifier, reweight training set according to prediction error
• Repeat
• Perform classification based on weighted vote of the classifiers

C-DAC/Kolkata 88

www.cdac.in



Deep vs Shallow Networks
Deep vs Shallow Networks

• Deeper networks perform better than shallow networks

– But only up to some limit: after a certain number of layers, the performance 
of deeper networks plateaus

output

Shallow Deep
NN NN

……

x x
1 x2 …… N

input
C-DAC/Kolkata 89

Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Convolutional Neural Networks (CNNs)
Convolutional Neural Networks

• Convolutional neural networks (CNNs) were primarily designed for image 
data

• CNNs use a convolutional operator for extracting data features

– Allows parameter sharing

– Efficient to train

– Have less parameters than NNs with fully-connected layers

• CNNs are robust to spatial translations of objects in images

• A convolutional filter slides (i.e., convolves) across the image

Convolutional 
Input matrix 3x3 filter

C-DAC/Kolkata 90
Picture from: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution

www.cdac.in



Convolutional Neural Networks (CNNs)
Convolutional Neural Networks

• When the convolutional filters are scanned over the image, they capture useful 
features

– E.g., edge detection by convolutions
0   1  0

Filter 1  -4  1
0   1  0

1 1 1 1 1 1 0.015686 0.015686 0.011765 0.015686 0.015686 0.015686 0.015686 0.964706 0.988235 0.964706 0.866667 0.031373 0.023529 0.007843
0.007843 0.741176 1 1 0.984314 0.023529 0.019608 0.015686 0.015686 0.015686 0.011765 0.101961 0.972549 1 1 0.996078 0.996078 0.996078 0.058824 0.015686
0.019608 0.513726 1 1 1 0.019608 0.015686 0.015686 0.015686 0.007843 0.011765 1 1 1 0.996078 0.031373 0.015686 0.019608 1 0.011765
0.015686 0.733333 1 1 0.996078 0.019608 0.019608 0.015686 0.015686 0.011765 0.984314 1 1 0.988235 0.027451 0.015686 0.007843 0.007843 1 0.352941
0.015686 0.823529 1 1 0.988235 0.019608 0.019608 0.015686 0.015686 0.019608 1 1 0.980392 0.015686 0.015686 0.015686 0.015686 0.996078 1 0.996078
0.015686 0.913726 1 1 0.996078 0.019608 0.019608 0.019608 0.019608 1 1 0.984314 0.015686 0.015686 0.015686 0.015686 0.952941 1 1 0.992157
0.019608 0.913726 1 1 0.988235 0.019608 0.019608 0.019608 0.039216 0.996078 1 0.015686 0.015686 0.015686 0.015686 0.996078 1 1 1 0.007843
0.019608 0.898039 1 1 0.988235 0.019608 0.015686 0.019608 0.968628 0.996078 0.980392 0.027451 0.015686 0.019608 0.980392 0.972549 1 1 1 0.019608
0.043137 0.905882 1 1 1 0.015686 0.035294 0.968628 1 1 0.023529 1 0.792157 0.996078 1 1 0.980392 0.992157 0.039216 0.023529

1 1 1 1 1 0.992157 0.992157 1 1 0.984314 0.015686 0.015686 0.858824 0.996078 1 0.992157 0.501961 0.019608 0.019608 0.023529
0.996078 0.992157 1 1 1 0.933333 0.003922 0.996078 1 0.988235 1 0.992157 1 1 1 0.988235 1 1 1 1
0.015686 0.74902 1 1 0.984314 0.019608 0.019608 0.031373 0.984314 0.023529 0.015686 0.015686 1 1 1 0 0.003922 0.027451 0.980392 1
0.019608 0.023529 1 1 1 0.019608 0.019608 0.564706 0.894118 0.019608 0.015686 0.015686 1 1 1 0.015686 0.015686 0.015686 0.05098 1
0.015686 0.015686 1 1 1 0.047059 0.019608 0.992157 0.007843 0.011765 0.011765 0.015686 1 1 1 0.015686 0.019608 0.996078 0.023529 0.996078
0.019608 0.015686 0.243137 1 1 0.976471 0.035294 1 0.003922 0.011765 0.011765 0.015686 1 1 1 0.988235 0.988235 1 0.003922 0.015686
0.019608 0.019608 0.027451 1 1 0.992157 0.223529 0.662745 0.011765 0.011765 0.011765 0.015686 1 1 1 0.015686 0.023529 0.996078 0.011765 0.011765
0.015686 0.015686 0.011765 1 1 1 1 0.035294 0.011765 0.011765 0.011765 0.015686 1 1 1 0.015686 0.015686 0.964706 0.003922 0.996078
0.007843 0.019608 0.011765 0.054902 1 1 0.988235 0.007843 0.011765 0.011765 0.015686 0.011765 1 1 1 0.015686 0.015686 0.015686 0.023529 1
0.007843 0.007843 0.015686 0.015686 0.960784 1 0.490196 0.015686 0.015686 0.015686 0.007843 0.027451 1 1 1 0.011765 0.011765 0.043137 1 1
0.023529 0.003922 0.007843 0.023529 0.980392 0.976471 0.039216 0.019608 0.007843 0.019608 0.015686 1 1 1 1 1 1 1 1 1

Input Image Convoluted 
Image

C-DAC/Kolkata 91
Slide credit: Param Vir Singh – Deep Learning

www.cdac.in



Convolutional Neural Networks (CNNs)
Convolutional Neural Networks

• In CNNs, hidden units in a layer are only connected to a small region of the 
layer before it (called local receptive field)

– The depth of each feature map corresponds to the number of 
convolutional filters used at each layer

w1 w2

w3 w4 w5 w6

w7 w8
Filter 1

Filter 2
Input Image

Layer 1 
Feature Map Layer 2 

Feature Map

C-DAC/Kolkata 92
Slide credit: Param Vir Singh – Deep Learning

www.cdac.in



Convolutional Neural Networks (CNNs)
Convolutional Neural Networks

• Max pooling: reports the maximum output within a rectangular neighborhood

• Average pooling: reports the average output of a rectangular neighborhood

• Pooling layers reduce the spatial size of the feature maps

– Reduce the number of parameters, prevent overfitting

MaxPool with a 2×2 filter with stride of 2

1 3 5 3
4 5

4 2 3 1
3 4

3 1 1 3

0 1 0 4
Output 
Matrix

Input 
Matrix

C-DAC/Kolkata 93
Slide credit: Param Vir Singh – Deep Learning

www.cdac.in



Convolutional Neural Networks (CNNs)
Convolutional Neural Networks

• Feature extraction architecture

– After 2 convolutional layers, a max-pooling layer reduces the size of the 
feature maps (typically by 2)

– A fully convolutional and a softmax layers are added last to perform 
classification

Living Room

Bedroom

Kitchen

Bathroom

Outdoor

Fully Connected Layer

C-DAC/Kolkata 94
Slide credit: Param Vir Singh – Deep Learning

64
64

128
128

256

256

256

Conv layer 512

512
512

Max Pool

512

512

512

www.cdac.in



Residual CNNs
Convolutional Neural Networks

• Residual networks (ResNets)

– Introduce “identity” skip 
connections

• Layer inputs are propagated 
and added to the layer output

• Mitigate the problem of 
vanishing gradients during 
training

• Allow training very deep NN 
(with over 1,000 layers)

– Several ResNet variants exist: 18, 
34, 50, 101, 152, and 200 layers

– Are used as base models of other 
state-of-the-art NNs 

• Other similar models: 
C-DAC/Kolkata 95

ResNeXT, DenseNet

www.cdac.in



Recurrent Neural Networks (RNNs)
Recurrent Neural Networks

• Recurrent NNs are used for modeling sequential data and data with varying 
length of inputs and outputs

– Videos, text, speech, DNA sequences, human skeletal data

• RNNs introduce recurrent connections between the neurons

– This allows processing sequential data one element at a time by selectively 
passing information across a sequence

– Memory of the previous inputs is stored in the model’s internal state and 
affect the model predictions

– Can capture correlations in sequential data

• RNNs use backpropagation-through-time for training

• RNNs are more sensitive to the vanishing gradient problem than CNNs

C-DAC/Kolkata 96

www.cdac.in



Recurrent Neural Networks (RNNs)
• RNN use same set of weights 𝑤ℎ and 𝑤𝑥 across all time steps

– A sequence of hidden states ℎ𝑜, ℎ𝑜ℎ2, ℎ3, … is learned, which 
represents the memory of the network

– The hidden state at step t, ℎ 𝑡 , is calculated based on the previous 
hidden state ℎ 𝑡 − 1 and the input at the current step 𝑥 𝑡 , i.e., ℎ 𝑡 =
𝑓ℎ 𝑤ℎ ∗ ℎ 𝑡 − 1 + 𝑤𝑥 ∗ 𝑥 𝑡

– The function 𝑓ℎ ∙ is a nonlinear activation function, e.g., ReLU or tanh

• RNN shown rolled over time
HIDDEN STATES SEQUENCE: OUTPUT

ℎ𝑜 , ℎ1, ℎ2, ℎ3, … .
𝑤ℎ 𝑤 𝑤 𝑤

ℎ ℎ 𝑦

h0 𝑓ℎ(·) h1 𝑓ℎ(·) h2 𝑓ℎ(·) 𝑓𝑦(·)
h3

𝑤𝑥 𝑤 𝑤
𝑥 𝑥

x1 x2 x3

INPUT SEQUENCE: 𝑥1, 𝑥C-DAC/Kolkata 2, 𝑥3, … . 97
Slide credit: Param Vir Singh – Deep Learning

www.cdac.in



Recurrent Neural Networks (RNNs)
Recurrent Neural Networks

• RNNs can have one of many inputs and one of many outputs

RNN Application Input Output

A person riding a 
Image 

motorbike on dirt 
Captioning

road

Sentiment Awesome movie. 
Analysis Highly Positive

recommended.

Machine 
Happy Diwali शुभ दीपावली

Translatio
n

C-DAC/Kolkata 98
Slide credit: Param Vir SIngh– Deep Learning

www.cdac.in



Bidirectional RNNs
Recurrent Neural Networks

• Bidirectional RNNs incorporate both forward and backward passes through 
sequential data

– The output may not only depend on the previous elements in the sequence,  
but also on future elements in the sequence

– It resembles two RNNs stacked on top of each other

ℎ𝑡 = 𝜎(𝑊(ℎℎ)ℎ𝑡−1 +𝑊(ℎ𝑥)𝑥𝑡)

ℎ𝑡 = 𝜎(𝑊(ℎℎ)ℎ𝑡+1 +𝑊(ℎ𝑥)𝑥𝑡)

𝑦𝑡 = 𝑓 ℎ𝑡; ℎ𝑡

Outputs both past and future elements

C-DAC/Kolkata 99
Slide credit: Param Vir Singh – Deep Learning

www.cdac.in



LSTM Networks
Recurrent Neural Networks

• Long Short-Term Memory (LSTM) networks are a variant of RNNs

• LSTM mitigates the vanishing/exploding gradient problem

– Solution: a Memory Cell, updated at each step in the sequence

• Three gates control the flow of information to and from the Memory Cell

– Input Gate: protects the current step from irrelevant inputs

– Output Gate: prevents current step from passing irrelevant information to 
later steps

– Forget Gate: limits information passed from one cell to the next

• Most modern RNN models use either LSTM units or other more advanced types 
of recurrent units (e.g., GRU units)

C-DAC/Kolkata 100

www.cdac.in



LSTM Networks
Recurrent Neural Networks

• LSTM cell

– Input gate, output gate, forget gate, memory cell 

– LSTM can learn long-term correlations within data sequences

C-DAC/Kolkata 101

www.cdac.in



References

1. Hung-yi Lee – Deep Learning Tutorial 

2. Ismini Lourentzou – Introduction to Deep Learning

3. CS231n Convolutional Neural Networks for Visual Recognition (Stanford CS 
course) (link)

4. James Hays, Brown – Machine Learning Overview

5. Param Vir Singh, Shunyuan Zhang, Nikhil Malik – Deep Learning

6. Sebastian Ruder – An Overview of Gradient Descent Optimization Algorithms 
(link)

C-DAC/Kolkata 102

www.cdac.in



THANK YOU

C-DAC/Kolkata 103

www.cdac.in



Elements of Neural Networks 
Introduction to Neural Networks

• A neural network playground link

C-DAC/Kolkata 104

www.cdac.in



Elements of Neural Networks 
Introduction to Neural Networks

• A simple network, toy example
1 ∙ 1 + −1 ∙ −2 + 1 = 4

1 4 0.98 Sigmoid Function

1
- 1

 z
2 1 1 ez

 z
-1 -2 0.12

-1
1 z

0

1 ∙ −1 + −1 ∙ 1 + 0 =-2
C-DAC/Kolkata 105

Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in



Elements of Neural Networks 
Introduction to Neural Networks

• A simple network, toy example (cont’d)

– For an input vector [1 −1]𝑇, the output is [0.62 0.83]𝑇

1 4 0.98 2 0.86 3 0.62
1

- - -
2 1 1 0 1 -

2
-1 -2 0.12 -2 0.11 -1 0.83

-1
1 -1 4

0 0 2

𝑓: 𝑅2 → 𝑅2 1 0.62
𝑓 =

−1 0.83
C-DAC/Kolkata 106

Slide credit: Hung-yi Lee – Deep Learning Tutorial

www.cdac.in